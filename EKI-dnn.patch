Index: recipes/naive_02_hts.cfg
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/recipes/naive_02_hts.cfg b/recipes/naive_02_hts.cfg
new file mode 100644
--- /dev/null	(revision 678eb3d913662bb307368afd5efd73ef8ee88c6e)
+++ b/recipes/naive_02_hts.cfg	(revision 678eb3d913662bb307368afd5efd73ef8ee88c6e)
@@ -0,0 +1,234 @@
+
+import sys
+import os
+import inspect
+from configobj import ConfigObj
+current_dir = os.path.realpath(os.path.abspath(os.path.dirname(inspect.getfile(inspect.currentframe()))))
+
+## for when config is still in recipes directory:
+sys.path.append(current_dir + '/../scripts/')
+sys.path.append(current_dir + '/../scripts/processors/')
+
+## for after config is copied to voice.cfg:
+sys.path.append(current_dir + '/../../../../scripts/')
+sys.path.append(current_dir + '/../../../../scripts/processors/')
+
+
+from Tokenisers import RegexTokeniser
+from Phonetisers import NaivePhonetiser
+from FeatureExtractor import WorldExtractor
+from FeatureDumper import FeatureDumper
+from Aligner import StateAligner
+from SKLProcessors import SKLDecisionTreePausePredictor 
+from PhraseMaker import PhraseMaker
+from AcousticModel import AcousticModelWorld
+from SimpleChildAdder import SimpleChildAdder
+
+import default.const as c
+
+
+
+
+
+
+
+## ----------------------------------------------------------------
+## First define a few things used later:
+
+## Some useful Xpaths and regex:--
+#CONTENT_NODES = "//token[@token_class='word'] | //token[@token_class='punctuation']"
+JUNCTURE_NODES = "//token[@token_class='space'] | //token[@token_class='punctuation']"
+
+LETTER_PATT = '[\p{L}||\p{N}||\p{M}]'
+PUNC_PATT = '[\p{C}||\p{P}||\p{S}]'
+SPACE_PATT = '\p{Z}'
+PUNC_OR_SPACE_PATT = '[\p{Z}||\p{C}||\p{P}||\p{S}]'
+
+## 
+speech_coding_config = {'order': 39, 'static_window': '1', 'delta_window': '-0.5 0.0 0.5', 'delta_delta_window': '1.0 -2.0 1.0'}
+
+
+pause_predictor_features = [        
+        ('response', './attribute::has_silence="yes"'), 
+        ('token_is_punctuation', './attribute::token_class="punctuation"'),
+        ('end_of_sentence', './attribute::token_class="%s"'%(c.TERMINAL)),         
+        ('since_start_utterance_in_words', "count(preceding::token[@token_class='word'])"),
+        ('till_end_utterance_in_words', "count(following::token[@token_class='word'])")
+]     
+
+        # L_vsm_d1 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d1
+        # L_vsm_d2 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d2
+        # L_vsm_d3 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d3
+        # L_vsm_d4 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d4
+        # L_vsm_d5 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d5
+        # L_vsm_d6 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d6
+        # L_vsm_d7 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d7
+        # L_vsm_d8 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d8
+        # L_vsm_d9 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d9
+        # L_vsm_d10 = ./preceding::token[@token_class!='space'][1]/attribute::word_vsm_d10
+
+        # C_vsm_d1 = ./attribute::word_vsm_d1
+        # C_vsm_d2 = ./attribute::word_vsm_d2
+        # C_vsm_d3 = ./attribute::word_vsm_d3
+        # C_vsm_d4 = ./attribute::word_vsm_d4
+        # C_vsm_d5 = ./attribute::word_vsm_d5
+        # C_vsm_d6 = ./attribute::word_vsm_d6
+        # C_vsm_d7 = ./attribute::word_vsm_d7
+        # C_vsm_d8 = ./attribute::word_vsm_d8
+        # C_vsm_d9 = ./attribute::word_vsm_d9
+        # C_vsm_d10 = ./attribute::word_vsm_d10
+        
+        # R_vsm_d1 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d1
+        # R_vsm_d2 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d2
+        # R_vsm_d3 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d3
+        # R_vsm_d4 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d4
+        # R_vsm_d5 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d5
+        # R_vsm_d6 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d6
+        # R_vsm_d7 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d7
+        # R_vsm_d8 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d8
+        # R_vsm_d9 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d9
+        # R_vsm_d10 = ./following::token[@token_class!='space'][1]/attribute::word_vsm_d10
+        
+
+
+    
+label_contexts = [
+        ## special named features:
+        ('htk_monophone', './attribute::pronunciation'),
+        ('start_time', './attribute::start'),
+        ('end_time', './attribute::end'),
+
+        ## normal features:
+        ('ll_segment', 'preceding::segment[2]/attribute::pronunciation'),
+        ('l_segment', 'preceding::segment[1]/attribute::pronunciation'),
+        ('c_segment', './attribute::pronunciation'),
+        ('r_segment', 'following::segment[1]/attribute::pronunciation'),
+        ('rr_segment', 'following::segment[2]/attribute::pronunciation'),
+
+        ## letter VSM features
+        # ('l_segment_vsm_d1 =      preceding::segment[1]/attribute::segment_vsm_d1
+        # ('l_segment_vsm_d2 =      preceding::segment[1]/attribute::segment_vsm_d2
+        # ('l_segment_vsm_d3 =      preceding::segment[1]/attribute::segment_vsm_d3
+        # l_segment_vsm_d4 =      preceding::segment[1]/attribute::segment_vsm_d4
+        # l_segment_vsm_d5 =      preceding::segment[1]/attribute::segment_vsm_d5
+        # c_segment_vsm_d1 =                          ./attribute::segment_vsm_d1
+        # c_segment_vsm_d2 =                          ./attribute::segment_vsm_d2
+        # c_segment_vsm_d3 =                          ./attribute::segment_vsm_d3
+        # c_segment_vsm_d4 =                          ./attribute::segment_vsm_d4
+        # c_segment_vsm_d5 =                          ./attribute::segment_vsm_d5
+        # r_segment_vsm_d1 =      following::segment[1]/attribute::segment_vsm_d1
+        # r_segment_vsm_d2 =      following::segment[1]/attribute::segment_vsm_d2
+        # r_segment_vsm_d3 =      following::segment[1]/attribute::segment_vsm_d3
+        # r_segment_vsm_d4 =      following::segment[1]/attribute::segment_vsm_d4
+        # r_segment_vsm_d5 =      following::segment[1]/attribute::segment_vsm_d5
+        
+        
+        ## WORD LEVEL:
+        ('length_left_word', "count(ancestor::token/preceding::token[@token_class='word'][1]/descendant::segment)"),
+        ('length_current_word', 'count(ancestor::token/descendant::segment)'),
+        ('length_right_word', "count(ancestor::token/following::token[@token_class='word'][1]/descendant::segment)"),
+
+        ('since_beginning_of_word', 'count(./preceding-sibling::segment)'),
+        ('till_end_of_word', 'count(./following-sibling::segment)'),
+
+        ## phrase LEVEL:
+        ('length_l_phrase_in_words', "count(ancestor::phrase/preceding::phrase[1]/descendant::token[@token_class='word'])"),
+        ('length_c_phrase_in_words', "count(ancestor::phrase/descendant::token[@token_class='word'])"),
+        ('length_r_phrase_in_words', "count(ancestor::phrase/following::phrase[1]/descendant::token[@token_class='word'])"),
+
+        ('length_l_phrase_in_segments', 'count(ancestor::phrase/preceding::phrase[1]/descendant::segment)'),
+        ('length_c_phrase_in_segments', 'count(ancestor::phrase/descendant::segment)'),
+        ('length_r_phrase_in_segments', 'count(ancestor::phrase/following::phrase[1]/descendant::segment)'),
+
+        ('since_phrase_start_in_segs', 'count(ancestor::token/preceding-sibling::token/descendant::segment)'),
+        ('till_phrase_end_in_segs', 'count(ancestor::token/following-sibling::token/descendant::segment)'),
+
+        ('since_phrase_start_in_words', "count(ancestor::token/preceding-sibling::token[@token_class='word'])"),
+        ('till_phrase_end_in_words', "count(ancestor::token/following-sibling::token[@token_class='word'])"),
+
+        ## UTT LEVEL:
+        ('since_start_utterance_in_segments', 'count(preceding::segment)'),
+        ('since_start_utterance_in_words', "count(preceding::token[@token_class='word'])"),
+        ('since_start_utterance_in_phrases', 'count(preceding::phrase)'),
+
+        ('till_end_utterance_in_segments', 'count(following::segment)'),
+        ('till_end_utterance_in_words', "count(following::token[@token_class='word'])"),
+        ('till_end_utterance_in_phrases', 'count(following::phrase)'),
+
+        ('length_utterance_in_segments', 'count(ancestor::utt/descendant::segment)'),
+        ('length_utterance_in_words', "count(ancestor::utt/descendant::token[@token_class='word'])"),
+        ('length_utterance_in_phrases', 'count(ancestor::utt/descendant::phrase)')
+]
+
+
+
+## ----------------------------------------------------------------
+## Now, a number of utterance processors are defined:--
+
+tokeniser = RegexTokeniser('word_splitter', target_nodes='//utt', split_attribute='text', \
+                            child_node_type='token', add_terminal_tokens=True, \
+                            class_patterns = [('space', '\A'+SPACE_PATT+'+\Z'), ('punctuation', '\A'+PUNC_OR_SPACE_PATT+'+\Z')], \
+                            split_pattern='('+SPACE_PATT+'*'+PUNC_PATT+'*'+SPACE_PATT+'+|'+SPACE_PATT+'*'+PUNC_PATT+'+\Z)'  )
+                                                    ## modified to handle word-internal hyphens
+
+# phonetiser = RegexTokeniser('letter_splitter', target_nodes="//token[@token_class='word']", split_attribute='text', \
+#                             child_node_type='segment', add_terminal_tokens=False, add_safetext=True, \
+#                             split_pattern = '(.)', add_token_classes=False )
+
+phonetiser = NaivePhonetiser('segment_adder', target_nodes="//token", target_attribute='text', child_node_type='segment', \
+                            class_attribute='token_class', output_attribute='pronunciation', word_classes = ['word'], \
+                            probable_pause_classes = ['punctuation', c.TERMINAL], possible_pause_classes=['space'])
+
+speech_feature_extractor = WorldExtractor('acoustic_feature_extractor', input_filetype='wav', output_filetype='cmp', \
+                            coding_config=speech_coding_config, sample_rate=16000, alpha=0.42, mcep_order=39)
+
+align_label_dumper = FeatureDumper(target_nodes='//segment', output_filetype='align_lab', contexts=[('segment', './attribute::pronunciation')])
+ 
+aligner = StateAligner(target_nodes='//segment', target_attribute='pronunciation', input_label_filetype='align_lab', acoustic_feature_filetype='cmp', \
+                    output_label_filetype='time_lab', silence_tag='has_silence', min_silence_duration=50, viterbi_beam_width='1000 100000 1000000', \
+                    acoustic_subrecipe='standard_alignment')
+
+pause_predictor = SKLDecisionTreePausePredictor(processor_name='pause_predictor', target_nodes=JUNCTURE_NODES, output_attribute='silence_predicted', contexts=pause_predictor_features)
+        
+phrase_adder = PhraseMaker(node_type_to_regroup='token', parent_node_type='phrase', \
+                 attribute_with_silence='pronunciation', silence_symbol='sil')
+
+label_maker = FeatureDumper(processor_name='labelmaker', target_nodes='//segment', context_separators='numbers', output_filetype='lab', \
+                question_file='questions.hed', question_filter_threshold = 5, contexts=label_contexts)
+
+acoustic_model = AcousticModelWorld(acoustic_subrecipe='quick_voicebuild_01', input_label_filetype='lab', \
+    acoustic_feature_filetype='cmp', output_filetype='wav', vuv = 0.51, speech_coding_config=speech_coding_config, sample_rate=16000, alpha=0.42, mcep_order=39,  \
+    training_settings={'NREEST': 3, 'NRECLUSTER': 2, 'BINMOD': " -B ", 'SUBSET': '60minutes'})
+
+
+def verify(cls, _):
+    return
+
+setattr(SimpleChildAdder, 'verify', classmethod(verify))
+        
+silenceConfig = ConfigObj()
+silenceConfig['target_nodes'] = '//token[@token_class="_END_"]'
+silenceConfig['child_tag'] = 'segment'
+silenceConfig['child_attribute'] = 'pronunciation'
+silenceConfig['child_attribute_value'] = 'sil'
+end_silence_adder = SimpleChildAdder('end_silence_adder', silenceConfig, None)
+
+## -----------------------------------------------------------------
+## The processors are grouped for convenience into several 'stages':
+
+text_proc = [tokeniser, phonetiser]
+alignment = [align_label_dumper, speech_feature_extractor, aligner, pause_predictor, phrase_adder]
+#pause_prediction = [pause_predictor, phrase_adder, end_silence_adder] 
+pause_prediction = [end_silence_adder, pause_predictor, phrase_adder]
+speech_generation = [label_maker, acoustic_model]
+
+
+
+## ----------------------------------------------------------------
+## The final part of the config specifies which stages are run in each of the modes
+## "train" and "runtime" (and optionally extra, specialised, modes):
+
+train_stages   = [text_proc, alignment,        speech_generation]
+
+runtime_stages = [text_proc, pause_prediction, speech_generation]
+
Index: recipes/naive_02_nn.cfg
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/recipes/naive_02_nn.cfg b/recipes/naive_02_nn.cfg
new file mode 100644
--- /dev/null	(revision 678eb3d913662bb307368afd5efd73ef8ee88c6e)
+++ b/recipes/naive_02_nn.cfg	(revision 678eb3d913662bb307368afd5efd73ef8ee88c6e)
@@ -0,0 +1,224 @@
+#!/usr/bin/env python
+
+import sys
+import os
+import inspect
+from configobj import ConfigObj
+current_dir = os.path.realpath(os.path.abspath(os.path.dirname(inspect.getfile(inspect.currentframe()))))
+
+## for when config is still in recipes directory:
+sys.path.append(current_dir + '/../scripts/')
+sys.path.append(current_dir + '/../scripts/processors/')
+
+## for after config is copied to voice.cfg:
+sys.path.append(current_dir + '/../../../../scripts/')
+sys.path.append(current_dir + '/../../../../scripts/processors/')
+
+from Tokenisers import RegexTokeniser
+from Phonetisers import NaivePhonetiser
+from VSMTagger import VSMTagger
+from FeatureExtractor import WorldExtractor
+from FeatureDumper import FeatureDumper
+from Aligner import StateAligner
+from SKLProcessors import SKLDecisionTreePausePredictor 
+from PhraseMaker import PhraseMaker
+from AcousticModel import AcousticModelWorld
+from NN import NNDurationPredictor, NNAcousticPredictor
+from SimpleChildAdder import SimpleChildAdder
+
+import default.const as c
+
+
+
+
+
+
+
+## ----------------------------------------------------------------
+## First define a few things used later:
+
+## Some useful Xpaths and regex:--
+#CONTENT_NODES = "//token[@token_class='word'] | //token[@token_class='punctuation']"
+JUNCTURE_NODES = "//token[@token_class='space'] | //token[@token_class='punctuation']"
+
+LETTER_PATT = '[\p{L}||\p{N}||\p{M}]'
+PUNC_PATT = '[\p{C}||\p{P}||\p{S}]'
+SPACE_PATT = '\p{Z}'
+PUNC_OR_SPACE_PATT = '[\p{Z}||\p{C}||\p{P}||\p{S}]'
+
+
+tokenisation_pattern = '('+SPACE_PATT+'*'+PUNC_PATT+'*'+SPACE_PATT+'+|'+SPACE_PATT+'*'+PUNC_PATT+'+\Z)'
+                                 ## modified to handle word-internal hyphens
+
+word_vsm_dim = 10
+
+speech_coding_config = {'order': 39, 'static_window': '1', 'delta_window': '-0.5 0.0 0.5', 'delta_delta_window': '1.0 -2.0 1.0'}
+
+
+
+## Define various context features used by processors:
+
+pause_predictor_features = [        
+        ('response', './attribute::has_silence="yes"'), 
+        ('token_is_punctuation', './attribute::token_class="punctuation"'),      
+        ('since_start_utterance_in_words', "count(preceding::token[@token_class='word'])"),
+        ('till_end_utterance_in_words', "count(following::token[@token_class='word'])")
+]     
+## add word VSM features to pause predictor:
+for dim in range(1, word_vsm_dim+1):
+    pause_predictor_features.append(('L_vsm_d%s'%(dim), "./preceding::token[@token_class='word'][1]/attribute::vsm_d%s"%(dim)))
+    pause_predictor_features.append(('C_vsm_d%s'%(dim), "./attribute::vsm_d%s"%(dim)))
+    pause_predictor_features.append(('R_vsm_d%s'%(dim), "./following::token[@token_class='word'][1]/attribute::vsm_d%s"%(dim)))
+
+
+## These can be used from either the state or phone level:
+phone_and_state_contexts = [
+    ## WORD LEVEL:
+    ("length_left_word", "count(ancestor::token/preceding::token[@token_class='word'][1]/descendant::segment)"),
+    ("length_current_word", "count(ancestor::token/descendant::segment)"),
+    ("length_right_word", "count(ancestor::token/following::token[@token_class='word'][1]/descendant::segment)"),
+    ("since_beginning_of_word", "count_Xs_since_start_Y('segment', 'token')"),
+    ("till_end_of_word", "count_Xs_till_end_Y('segment', 'token')"),
+
+    ## phrase LEVEL:
+    ("length_l_phrase_in_words", "count(ancestor::phrase/preceding::phrase[1]/descendant::token[@token_class='word'])"),
+    ("length_c_phrase_in_words", "count(ancestor::phrase/descendant::token[@token_class='word'])"),
+    ("length_r_phrase_in_words", "count(ancestor::phrase/following::phrase[1]/descendant::token[@token_class='word'])"),
+
+    ("length_l_phrase_in_segments", "count(ancestor::phrase/preceding::phrase[1]/descendant::segment)"),
+    ("length_c_phrase_in_segments", "count(ancestor::phrase/descendant::segment)"),
+    ("length_r_phrase_in_segments", "count(ancestor::phrase/following::phrase[1]/descendant::segment)"),
+
+    ("since_phrase_start_in_segs", "count_Xs_since_start_Y('segment', 'phrase')"),
+    ("till_phrase_end_in_segs", "count_Xs_till_end_Y('segment', 'phrase')"),
+
+    ("since_phrase_start_in_words", "count_Xs_since_start_Y('token[@token_class=\"word\"]', 'phrase')"),
+    ("till_phrase_end_in_words", "count_Xs_till_end_Y('token[@token_class=\"word\"]', 'phrase')"),
+
+    ## SENTENCE (UTT) LEVEL
+    ("since_start_sentence_in_segments", "count_Xs_since_start_Y('segment', 'utt')"),
+    ("since_start_sentence_in_words", "count_Xs_since_start_Y('token[@token_class=\"word\"]', 'utt')"),
+    ("since_start_sentence_in_phrases", "count_Xs_since_start_Y('phrase', 'utt')"),
+    ("till_end_sentence_in_segments", "count_Xs_till_end_Y('segment', 'utt')"),
+    ("till_end_sentence_in_words", "count_Xs_till_end_Y('token[@token_class=\"word\"]', 'utt')"),
+    ("till_end_sentence_in_phrases", "count_Xs_till_end_Y('phrase', 'utt')"),
+    ("length_sentence_in_segments", "count(ancestor::utt/descendant::segment)"),
+    ("length_sentence_in_words", "count(ancestor::utt/descendant::token[@token_class='word'])"),
+    ("length_sentence_in_phrases", "count(ancestor::utt/descendant::phrase)")
+]
+
+## add word VSM features:
+for dim in range(1, word_vsm_dim+1):
+    phone_and_state_contexts.append(('L_vsm_d%s'%(dim), "./ancestor::token/preceding::token[@token_class='word'][1]/attribute::vsm_d%s"%(dim)))
+    phone_and_state_contexts.append(('C_vsm_d%s'%(dim), "./ancestor::token/attribute::vsm_d%s"%(dim)))
+    phone_and_state_contexts.append(('R_vsm_d%s'%(dim), "./ancestor::token/following::token[@token_class='word'][1]/attribute::vsm_d%s"%(dim)))
+
+
+
+phone_contexts = [
+        ## special named features:
+        ('htk_monophone', './attribute::pronunciation'),
+        ('start_time', './attribute::start'),
+        ('end_time', './attribute::end'),
+
+        ## normal features:
+        ('ll_segment', 'preceding::segment[2]/attribute::pronunciation'),
+        ('l_segment', 'preceding::segment[1]/attribute::pronunciation'),
+        ('c_segment', './attribute::pronunciation'),
+        ('r_segment', 'following::segment[1]/attribute::pronunciation'),
+        ('rr_segment', 'following::segment[2]/attribute::pronunciation')      
+] + phone_and_state_contexts
+
+
+state_contexts = [
+    ## special named features:
+    ("start_time", "./attribute::start"),
+    ("end_time", "./attribute::end"),
+    ("htk_state", "count(./preceding-sibling::state) + 1"),
+    ("htk_monophone", "./ancestor::segment/attribute::pronunciation"),
+
+    ("ll_segment", "./ancestor::segment/preceding::segment[2]/attribute::pronunciation"),
+    ("l_segment", "./ancestor::segment/preceding::segment[1]/attribute::pronunciation"),
+    ("c_segment", "./ancestor::segment/attribute::pronunciation"),
+    ("r_segment", "./ancestor::segment/following::segment[1]/attribute::pronunciation"),
+    ("rr_segment", "./ancestor::segment/following::segment[2]/attribute::pronunciation")
+] + phone_and_state_contexts
+
+## This is a set of Xpaths for obtaining state durations:-
+duration_data_contexts = [('state_%s_nframes'%(i), '(./state[%s]/attribute::end - ./state[%s]/attribute::start) div 5'%(i,i)) for i in [1,2,3,4,5]]
+
+
+## ----------------------------------------------------------------
+## Now, a number of utterance processors are defined:--
+
+tokeniser = RegexTokeniser('word_splitter', target_nodes='//utt', split_attribute='text', \
+                            child_node_type='token', add_terminal_tokens=True, \
+                            class_patterns = [('space', '\A'+SPACE_PATT+'+\Z'), ('punctuation', '\A'+PUNC_OR_SPACE_PATT+'+\Z')], \
+                            split_pattern=tokenisation_pattern)
+                                                
+
+phonetiser = NaivePhonetiser('segment_adder', target_nodes="//token", target_attribute='text', child_node_type='segment', \
+                            class_attribute='token_class', output_attribute='pronunciation', word_classes = ['word'], \
+                            probable_pause_classes = ['punctuation', c.TERMINAL], possible_pause_classes=['space'])
+
+word_vector_tagger = VSMTagger('word_vector_tagger', target_nodes="//token[@token_class='word']", input_attribute='text', \
+                    output_attribute_stem='vsm', norm_counts=True, svd_algorithm='randomized',  context_size=250, rank=word_vsm_dim, \
+                    unseen_method=5, discretisation_method='none', tokenisation_pattern=tokenisation_pattern)
+
+speech_feature_extractor = WorldExtractor('acoustic_feature_extractor', input_filetype='wav', output_filetype='cmp', \
+                            coding_config=speech_coding_config, sample_rate=16000, alpha=0.42, mcep_order=39)
+
+align_label_dumper = FeatureDumper(target_nodes='//segment', output_filetype='align_lab', contexts=[('segment', './attribute::pronunciation')])
+ 
+aligner = StateAligner(target_nodes='//segment', target_attribute='pronunciation', input_label_filetype='align_lab', acoustic_feature_filetype='cmp', \
+                    output_label_filetype='time_lab', silence_tag='has_silence', min_silence_duration=50, viterbi_beam_width='1000 100000 1000000', \
+                    acoustic_subrecipe='standard_alignment')
+
+pause_predictor = SKLDecisionTreePausePredictor(processor_name='pause_predictor', target_nodes=JUNCTURE_NODES, output_attribute='silence_predicted', contexts=pause_predictor_features)
+        
+phrase_adder = PhraseMaker(node_type_to_regroup='token', parent_node_type='phrase', \
+                 attribute_with_silence='pronunciation', silence_symbol='sil')
+
+dur_data_maker = FeatureDumper(processor_name='duration_data_maker', target_nodes='//segment', context_separators='spaces', output_filetype='dur', \
+                question_file='null', contexts=duration_data_contexts, binary_output=True)
+
+dur_label_maker = FeatureDumper(processor_name='labelmaker', target_nodes='//segment', context_separators='numbers', output_filetype='lab_dur', \
+                question_file='questions_dur.hed', question_filter_threshold = 5, contexts=phone_contexts)
+
+duration_predictor = NNDurationPredictor(processor_name='duration_predictor', question_file='questions_dur.hed', target_nodes='//segment')
+
+dnn_label_maker = FeatureDumper(processor_name='dnn_labelmaker', target_nodes='//state', context_separators='numbers', output_filetype='lab_dnn', \
+                question_file='questions_dnn.hed', question_filter_threshold=5, contexts=state_contexts)
+
+acoustic_predictor = NNAcousticPredictor(variance_expansion=0.3, sample_rate=16000, alpha=0.42, mcep_order=39, input_label_filetype='lab_dnn')
+
+def verify(cls, _):
+    return
+
+setattr(SimpleChildAdder, 'verify', classmethod(verify))
+        
+silenceConfig = ConfigObj()
+silenceConfig['target_nodes'] = '//token[@token_class="_END_"]'
+silenceConfig['child_tag'] = 'segment'
+silenceConfig['child_attribute'] = 'pronunciation'
+silenceConfig['child_attribute_value'] = 'sil'
+end_silence_adder = SimpleChildAdder('end_silence_adder', silenceConfig, None)
+
+## -----------------------------------------------------------------
+## The processors are grouped for convenience into several 'stages':
+
+text_proc = [tokeniser, phonetiser, word_vector_tagger]
+alignment = [align_label_dumper, speech_feature_extractor, aligner, pause_predictor, phrase_adder, dur_data_maker]
+pause_prediction = [end_silence_adder, pause_predictor, phrase_adder] 
+speech_generation = [dur_label_maker, duration_predictor, dnn_label_maker, acoustic_predictor]
+
+
+
+## ----------------------------------------------------------------
+## The final part of the config specifies which stages are run in each of the modes
+## "train" and "runtime" (and optionally extra, specialised, modes):
+
+train_stages   = [text_proc, alignment,        speech_generation]
+
+runtime_stages = [text_proc, pause_prediction, speech_generation]
+
Index: scripts/main/Utterance.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/main/Utterance.py b/scripts/main/Utterance.py
--- a/scripts/main/Utterance.py	(revision fd01c8f9e1e5fa4f4f00dd444a565b714973b7a9)
+++ b/scripts/main/Utterance.py	(revision 678eb3d913662bb307368afd5efd73ef8ee88c6e)
@@ -29,9 +29,7 @@
 
 from distutils.spawn import find_executable   # to check if required executables are available
 
-
-
-
+from processors.phoneConversions import conversions
 
 class UtteranceElement(etree.ElementBase):
     """
@@ -233,7 +231,7 @@
                 string_data = text[0]
             else:
                 string_data = ' '.join(text)
-                
+            string_data = conversions(string_data)
 
             self.make_from_string(string_data, speech_file=speech_file)
             try:
Index: scripts/processors/phoneConversions.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/processors/phoneConversions.py b/scripts/processors/phoneConversions.py
new file mode 100644
--- /dev/null	(revision 678eb3d913662bb307368afd5efd73ef8ee88c6e)
+++ b/scripts/processors/phoneConversions.py	(revision 678eb3d913662bb307368afd5efd73ef8ee88c6e)
@@ -0,0 +1,36 @@
+# -*- coding: utf-8 -*-
+import re
+
+def conversions(contents):
+    beginning_patterns = ['ce', 'ci', 'c', 'qu', 'q', 'w', 'x', 'y', 'zz', 'z', 'ž',
+                          'Ce', 'Ci', 'C', 'Qu', 'Q', 'W', 'X', 'Y', 'Zz', 'Z', 'Ž',
+                          'üüa', 'üüe', 'üüo', 'üüu', 'üüja',
+                          '  ']
+    beginning_patterns = [item.decode('utf-8') for item in beginning_patterns]
+    new_patterns = ['tse', 'tsi', 'k', 'kv', 'k', 'v', 'ks', 'i', 'ts', 's', 'š',
+                    'Tse', 'Tsi', 'K', 'Kv', 'K', 'V', 'Ks', 'I', 'Ts', 'S', 'Š',
+                    'üija', 'üije', 'üijo', 'üiju', 'üija',
+                    ' ']
+    new_patterns = [item.decode('utf-8') for item in new_patterns]
+
+    i = 0
+    for beginning_pattern in beginning_patterns:
+        pattern = re.compile(beginning_pattern)
+        contents = pattern.sub(new_patterns[i], contents)
+        i += 1
+
+    vowels_i_patterns = re.compile(r'[aeiouõäöüAEIOUÕÄÖÜ]'.decode('utf-8')+'i'.decode('utf-8')+'[aeiouõäöü]'.decode('utf-8'))
+    patterns = vowels_i_patterns.finditer(contents)
+    i = 0
+    for pattern in patterns:
+        contents = contents[:pattern.start()+2+i]+'j'+contents[pattern.start()+2+i:]
+        i += 1
+
+    vowels_u_patterns = re.compile(r'[aeiouõäöüAEIOUÕÄÖÜ]'.decode('utf-8') + 'u'.decode('utf-8') + '[aeiouõäöü]'.decode('utf-8'))
+    patterns = vowels_u_patterns.finditer(contents)
+    i = 0
+    for pattern in patterns:
+        contents = contents[:pattern.start() + 2 + i] + 'w' + contents[pattern.start() + 2 + i:]
+        i += 1
+
+    return contents
\ No newline at end of file
Index: scripts/speak.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/speak.py b/scripts/speak.py
--- a/scripts/speak.py	(revision fd01c8f9e1e5fa4f4f00dd444a565b714973b7a9)
+++ b/scripts/speak.py	(revision 678eb3d913662bb307368afd5efd73ef8ee88c6e)
@@ -21,6 +21,8 @@
 
 import timeit
 
+from processors.phoneConversions import conversions
+
 def start_clock(comment):
     print '%s... '%(comment),
     return (timeit.default_timer(), comment)
@@ -103,7 +105,8 @@
     for line in fileinput.input(opts.files):
        line = line.decode('utf-8').rstrip()
        t = start_clock('Synthesise sentence')
-       print line
+       line = conversions(line)
+       print line.encode('utf-8')
        if fileinput.isfirstline():
            if para != []:
                voice.synth_utterance(''.join(para), output_wavefile=output_wavefile, \
